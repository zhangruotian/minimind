# PPO 

目录：

1. 大图：在 LLM 里 RLHF + PPO 是干嘛的  
2. 符号和概念：$\pi$、 $V$、 $r$、 $R$、 $R_t$、 $A_t$  
3. 多 token 场景下的时间线 & 状态定义  
4. Critic（V）的含义：不是 reward，而是“未来总价值”  
5. 怎么算每一步的 Target $R_t$：为什么不能都用同一个 $R_{\text{final}}$  
6. Advantage $A_t$：公式、含义、作用  
7. Critic 怎么训练（Loss 是什么，为什么这样）  
8. Actor 怎么训练（和 CE 的关系，A 如何控制方向和力度）  
9. “Boil → the → shoe” 例子：从头走一遍  
10. 小结：复习时可以记住的核心要点  
11. 代码实现解析：PPO Loss 的计算

---

## 1. 大图：在 LLM 里 RLHF + PPO 是干嘛的？

大模型训练三步：

1. **预训练**：在海量文本上做 next-token prediction。  
2. **SFT（监督微调）**：模仿高质量对话 / demo。  
3. **RLHF（用人类偏好做强化学习）**：  
   - 训练一个 **reward model** 给回答打分；  
   - 用强化学习（常用 **PPO**）调整 LLM，使输出更符合人类偏好。

在 RLHF 中：

- **Actor**：LLM（策略网络），参数 $\theta$。  
- **Critic**：价值网络，参数 $\phi$。  
- **Reward model**：打分器，参数 $\psi$（一般已训练好并冻结）。

PPO 的作用：  
> 更新 Actor，让其朝“高奖励”的方向移动，同时**限制每次更新幅度**，避免训练不稳定或模型崩坏。

---

## 2. 符号和概念统一

考虑一条对话轨迹：一个 prompt → 一段回答。

- prompt：记作 $q$。  
- 生成的 token 序列：$y_1, y_2, \dots, y_T$。

### 2.1 状态（state）和动作（action）

用“前缀作为状态”的定义（在 LLM 中很直观）：

- $s_1 = q + y_1$（如 `Question + Boil`）  
- $s_2 = q + y_1 + y_2$（如 `Question + Boil + the`）  
- …  
- $s_T = q + y_1 + \dots + y_T$  

每一步的 **动作** $a_t$ 就是当前生成的 token：  
$$
a_t = y_t
$$

### 2.2 策略（Actor）与价值（Critic）

- **策略 / Actor / Policy**：  
  $$
  \pi_\theta(a_t \mid s_t)
  $$  
  在状态 $s_t$ 下生成 token $a_t$ 的概率。

- **价值 / Critic / Value**：  
  $$
  V_\phi(s_t)
  $$  
  在看到当前前缀 $s_t$ 时，从现在到结束**预期能拿到的总奖励**。

直觉：

> $V(s_t)$ 表示“当前句子写到这里，这局大概率能拿几分？”

### 2.3 reward 和 return

- **即时 reward**：$r_t$  
  - RLHF 中通常只有最后一步有“大 reward”（来自 reward model）；  
  - 前面步骤的 $r_1,\dots,r_{T-1}$ 通常为 0 或仅包含 KL 惩罚。

- **终局得分**：  
  $$
  R_{\text{final}}
  $$  
  例如 reward model 对整段回答给出的打分（可能再减去 KL penalty）。

- **Return / return-to-go（从某一步往后看的总价值）**：  
  $$
  G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k
  $$  
  RLHF 常用 $\gamma \approx 1$。

**Critic 想要学习的是：**

$$
V(s_t) \approx \mathbb{E}[G_t \mid s_t]
$$

---

## 3. 多 token 场景下的时间线示例

以简化句子为例：

> `Question → Boil → the → shoe`

设：

- $s_1 =$ `Question + Boil`  
- $s_2 =$ `Question + Boil + the`  
- $s_3 =$ `Question + Boil + the + shoe`（终局）

假设 Critic 当前估计：

- $V(s_1) = 8$（写到 Boil：8 分）  
- $V(s_2) = 9$（写到 Boil the：9 分，更乐观）  
- $V(s_3) = 1$（写到 Boil the shoe：1 分，觉得要完）

reward model 最终给出的分数：

$$
R_{\text{final}} = 0
$$

接下来要用这个结果训练 Critic 和 Actor，并避免“所有步骤都被同一 0 分惩罚”的问题。

---

## 4. Critic 的含义：不是 reward，而是“未来总价值”

再次强调：

- **即时 reward** $r_t$：某一步的即时得分。  
- **价值** $V(s_t)$：在状态 $s_t$ 时，从现在到结束**预期能获得的总分**。

在多步环境下：

- 某一步“好不好”不仅看这一步有没有即时 reward，还取决于**它把你带向什么未来状态**；  
- 所以 $V(s_t)$ 是“从这里出发这局整体能拿多少分的平均值”，而不是“当前这一步拿几分”。

---

## 5. 每一步 Target $R_t$ 怎么算？为什么不能直接用统一的 $R_{\text{final}}$？

### 5.1 过度简化（错误）做法

错误思路：

> “每一步的 $R_t$ 都等于同一个最终得分 $R_{\text{final}}$。”

即：

$$
R_t = R_{\text{final}},\quad \forall t
$$

则：

$$
A_t = R_t - V(s_t) = R_{\text{final}} - V(s_t)
$$

如果最终 $R_{\text{final}} = 0$，则全部 $A_t \le 0$，所有步骤都被惩罚：

- 即使早期的动作实际是**把局势变好**的，也会因为最后一步踢飞被连坐惩罚——“Boil 被冤枉”。

这种做法只能用于非常粗糙的直觉解释，不适合真正的实现。

### 5.2 正确思路：TD / GAE，自举（Bootstrapping）

核心思想：

> **越接近结局的状态，信息越多、估计越准**，  
> 所以前一时刻的 target 要用后一时刻的价值来“教”。

最简单的 TD(0) 形式：

$$
R_t \approx r_t + \gamma V_{\text{old}}(s_{t+1})
$$

解释：

- 当前时刻 $t$ 不知道真实未来，  
- 用下一时刻 $s_{t+1}$ 的价值估计 $V(s_{t+1})$ 当“老师”，  
- 把“更接近终局的、更准的估计”向前传递（自举）。

在 RLHF 中：

- 通常前几步 $r_t \approx 0$，  
- 最后一两步包含主要 reward（终局得分 + KL 惩罚等）；
- 所以前几步常近似为：

$$
R_t \approx V_{\text{old}}(s_{t+1})
$$

**GAE（Generalized Advantage Estimation）** 在此基础上用多步 TD 做平滑和加权，本质仍是：

> 用“后面的 $V$ + 真实 $r$”构造每一步的“未来总价值” target。

---

## 6. Advantage：$A_t = R_t - V(s_t)$

定义：

$$
A_t = R_t - V(s_t)
$$

含义：

- $R_t$：从这一步往后看的“现实价值”（通过 TD/GAE 构造，包含未来信息）；  
- $V(s_t)$：当时的预测；  
- $A_t$ = 实际（现实） − 预期 = **“惊喜（surprise）”**：

  - $A_t > 0$：现实比预期好 → 该动作值得奖励；  
  - $A_t < 0$：现实比预期差 → 该动作需要惩罚。

优势：

- 把每一步对最终结果的“功劳 / 锅”拆分出来；  
- 有利于避免“最后一步踢飞 → 全员连坐”的情况，  
  保护早期真正有贡献的好动作。

---

## 7. Critic 怎么训练？

目标：

> 对每个状态 $s_t$，让 $V_\phi(s_t)$ 拟合构造出的目标 $R_t$（future reward）。

常用 Loss（MSE）：

$$
L_{\text{critic}}(\phi)
= \mathbb{E}_t \big[(V_\phi(s_t) - R_t)^2\big]
$$

- 如果使用 MC（蒙特卡洛）：$R_t = G_t$（整条轨迹的 return）；  
- 如果使用 TD / GAE：$R_t$ 是由 $r_t$ 和后续 $V$ 混合构造出来的目标。

训练效果：

- 如果某一步之后，现实情况远差于当时的预期：  
  $R_t \ll V(s_t)$ → loss 大 → 迫使 Critic 下调那一步的估计；  
- 若现实优于预期：  
  $R_t \gg V(s_t)$ → 迫使 Critic 上调估计。

---

## 8. Actor 怎么训练？与 CE 的关系

Actor 的目标：

> 让“advantage 大的动作”出现得更多，“advantage 负且绝对值大的动作”出现得更少。

基本 policy gradient 形式：

$$
L_{\text{actor}}(\theta)
= -\mathbb{E}_t[A_t \log \pi_\theta(a_t \mid s_t)]
$$

与纯 CE（cross-entropy）对比：

$$
L_{\text{CE}} = -\sum_t \log \pi_\theta(y_t \mid s_t)
$$

可以理解为：

> 仍然是对 sampled token 做 CE/NLL，  
> 但每个 token 的 loss 前面乘了权重 $A_t$：

- $A_t > 0$：像普通 CE，一样提高该 token 的概率；  
- $A_t < 0$：loss 变号，反向更新，降低该 token 的概率；  
- $|A_t|$：控制更新幅度（力度）。

**PPO** 在此基础上增加：

$$
r_t(\theta)
= \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}
$$

$$
L_{\text{PPO}}
= -\mathbb{E}_t\Big[\min\big(r_t A_t,\; \text{clip}(r_t,1-\epsilon,1+\epsilon)A_t \big)\Big]
$$

作用：

- 若新策略概率变化过大（$r_t$ 偏离 1 太多），  
  用 clip 后的值限制更新，  
- 保证每步更新不至于太激进，训练更稳定。

---

## 9. “Boil → the → shoe” 例子：完整流程

设：

- $s_1 =$ `Question + Boil`  
- $s_2 =$ `Question + Boil + the`  
- $s_3 =$ `Question + Boil + the + shoe`（终局）

Critic 当前估计：

- $V(s_1) = 8$  
- $V(s_2) = 9$  
- $V(s_3) = 1$

Reward model 最终打分：

$$
R_{\text{final}} = 0
$$

### 9.1 Target $R_t$ 与 Advantage $A_t$（TD 风格）

**Step 3（shoe）之前：**

- 最后一步，直接用最终得分作为 target：
  $$
  R_3 = R_{\text{final}} = 0
  $$
- Advantage：
  $$
  A_3 = R_3 - V(s_3)
      = 0 - 1
      = -1
  $$
  含义：原本预测还有 1 分，结果为 0 分，略微失望 → 惩罚“shoe”。

---

**Step 2（the）之后，shoe 之前：**

- target 来自下一步的价值：
  $$
  R_2 \approx V(s_3) = 1
  $$
- Advantage：
  $$
  A_2 = R_2 - V(s_2)
      = 1 - 9
      = -8
  $$
  含义：写到 Boil the 时，Critic 以为能拿到 9 分；  
  下一步局势变成 1 分 → 巨大的负惊喜 → **主要“背锅”的是“在 Boil 后接 the”**。

---

**Step 1（Boil）之后，the 之前：**

- target 来自下一步的价值：
  $$
  R_1 \approx V(s_2) = 9
  $$
- Advantage：
  $$
  A_1 = R_1 - V(s_1)
      = 9 - 8
      = +1
  $$
  含义：从 s₁ 到 s₂，局势由 8 分变成 9 分，略有好转 → 轻微正向惊喜 → **Boil 得到奖励，而不是被冤枉惩罚**。

---

### 9.2 Critic 的训练

用 $R_t$ 作为目标：

$$
L_{\text{critic}}
= (V(s_3)-R_3)^2
+ (V(s_2)-R_2)^2
+ (V(s_1)-R_1)^2
$$

代入数值：

- Step 3：
  $$
  L_3 = (1 - 0)^2 = 1
  $$
- Step 2：
  $$
  L_2 = (9 - 1)^2 = 64
  $$
- Step 1：
  $$
  L_1 = (8 - 9)^2 = 1
  $$

优化后，Critic 会调整：

- $V(s_1) \to 9$：Boil 之后形势不错；  
- $V(s_2) \to 1$：Boil the 之后其实形势已经岌岌可危；  
- $V(s_3) \to 0$：Boil the shoe 之后基本凉了。

---

### 9.3 Actor 的训练

Actor 的 loss：

$$
L_{\text{actor}}
= - \big(
A_1 \log \pi_\theta(\text{Boil} \mid s_1)
+ A_2 \log \pi_\theta(\text{the} \mid s_2)
+ A_3 \log \pi_\theta(\text{shoe} \mid s_3)
\big)
$$

代入 advantage：

- Step 1（Boil）：$A_1 = +1$  
  → 类似普通 CE → 提高在 s₁ 下选择 Boil 的概率。

- Step 2（the）：$A_2 = -8$  
  → 大的负数 → 强烈减少在 s₂ 下选择 the 的概率（主要背锅）。

- Step 3（shoe）：$A_3 = -1$  
  → 适度减少在 s₃ 下选择 shoe 的概率。

结果：

- 即使最终得分是 0，  
- 也不会“一刀把所有步骤都惩罚”，  
- credit assignment 自动把主要责任分配给 the 和 shoe，  
- 保护了早期的 Boil 这类本来是正向贡献的动作。

---

## 10. 小结：复习时可记住的核心要点

1. **价值函数 $V(s)$ 不是即时 reward，而是未来总价值**
   $$
   V(s_t) \approx \mathbb{E}[G_t \mid s_t]
   $$

2. **每一步的 Target $R_t$ 不是统一的终局得分 $R_{\text{final}}$**  
   而是“从这一步往后看的未来价值”：
   $$
   R_t \approx r_t + \gamma V(s_{t+1}) \quad\text{（TD / GAE 思想）}
   $$

3. **Advantage：$A_t = R_t - V(s_t)$**  
   - 表示“现实（后验）比当时预期好多少 / 差多少”；  
   - $A_t > 0$：奖励该动作；  
   - $A_t < 0$：惩罚该动作；  
   - 保护早期好动作不因终局失败被误伤。

4. **Critic 的训练目标是拟合 $R_t$（future reward），不是拟合单步 reward**
   $$
   L_{\text{critic}} = \mathbb{E}_t[(V(s_t) - R_t)^2]
   $$

5. **Actor 的更新：CE/NLL + Advantage 权重**
   $$
   L_{\text{actor}} = -\mathbb{E}_t[A_t \log \pi_\theta(a_t \mid s_t)]
   $$  
   - Advantage 的符号决定“加概率还是减概率”；  
   - Advantage 的绝对值决定更新力度；  
   - PPO 再通过 ratio+clip 限制每步更新幅度，增强稳定性。

6. **“Boil → the → shoe” 例子帮助记忆 credit assignment：**
   - Boil：$A > 0$，被奖励；  
   - the：$A \ll 0$，主要背锅；  
   - shoe：$A < 0$，也背锅但相对次要。  

这正是 RLHF + PPO 中 actor–critic 框架希望实现的：  
让模型不仅看最终得分，还能合理地把“好处 / 责任”分配到每个 token 上。

---

## 11. 代码实现解析：PPO Loss 的计算

结合 `train_ppo.py` 中的代码实现，我们来看看理论是如何落地的。

**注意**：本代码库采用了**简化版**的实现：
- **Advantage 计算**：没有使用 GAE 或 per-step TD，而是直接用（最终 Reward - 最终 Value）作为一个整句的 Advantage。
- **Log Prob 计算**：是对整句 response 求和，而不是对每个 token 单独算 Loss。

尽管粒度变粗了，但 PPO 的核心逻辑（Importance Sampling Ratio + Clipping）保持不变。

### 11.1 核心逻辑回顾

理想的 Policy Gradient 形式：
$$
L_{\text{actor}} = -\mathbb{E}_t [ A_t \log \pi_\theta(a_t \mid s_t) ]
$$
形式上就是 Cross Entropy (CE)，只是每个样本乘了一个权重 $A_t$：
- $A_t > 0$：像普通 CE 一样，提高该动作的概率。
- $A_t < 0$：Loss 变号，相当于反向 CE，降低该动作的概率。

PPO 在此基础上引入了 `ratio` 和 `clip` 来限制更新幅度。

### 11.2 代码步骤详解

#### Step 1: 计算 Log Probability (Actor, Old Actor, Ref)

首先获取三个模型对同一生成序列的 log probability。

```python
# 当前 Actor 的 Log Prob
actor_logp = (logp_tokens * final_mask).sum(dim=1)  # [B]
# Old Actor（用于 PPO Clip）的 Log Prob
old_logp   = (old_logp_tokens * final_mask).sum(dim=1)  # [B]
# Reference Model（用于 KL Penalty）的 Log Prob
ref_logp   = (ref_logp_tokens * final_mask).sum(dim=1)  # [B]
```

- `actor_logp[i]`：当前策略 $\pi_\theta$ 对第 $i$ 个样本整个 response 序列的 log 概率总和。
- `old_logp`：来自上一轮（被冻结）的 Actor。
- `ref_logp`：来自 SFT 初始模型（始终冻结）。

#### Step 2: 计算 KL Divergence（监控与惩罚）

KL 散度（Kullback-Leibler Divergence）用于衡量两个概率分布的差异。
$$
\text{KL}(P \| Q) = \mathbb{E}_{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right] = \mathbb{E}_{x \sim P} [\log P(x) - \log Q(x)]
$$

```python
kl = (actor_logp - old_logp).mean()  # [Scalar] 用于监控
kl_ref = (actor_logp - ref_logp).mean()  # [Scalar] 用于 Loss
```

- **Why sum?** 从“每个 step 是一个随机变量”的视角来看 `.sum()`：
  - 对第 $i$ 条样本、第 $t$ 个 token（动作），记  
    $$
    X_t^{(i)} = \log \pi_{\text{new}}(a_t^{(i)} \mid s_t^{(i)}) - \log \pi_{\text{old}}(a_t^{(i)} \mid s_t^{(i)})
    $$
    这就是在 **该 step 上** 的 log-ratio 随机变量。
  - 代码里的
    $$
    \text{actor\_logp}_i = \sum_t \log \pi_{\text{new}}(a_t^{(i)} \mid s_t^{(i)}),\quad
    \text{old\_logp}_i   = \sum_t \log \pi_{\text{old}}(a_t^{(i)} \mid s_t^{(i)})
    $$
    因此
    $$
    \text{actor\_logp}_i - \text{old\_logp}_i
    = \sum_t \big(\log \pi_{\text{new}} - \log \pi_{\text{old}}\big)
    = \sum_t X_t^{(i)}
    $$
    即：对**这一条样本的所有 step 的 log-ratio 做了求和**。
  - 再在 batch 维度上 `.mean()`：
    $$
    \text{kl} = \frac{1}{B} \sum_{i=1}^B (\text{actor\_logp}_i - \text{old\_logp}_i)
              = \frac{1}{B} \sum_{i=1}^B \sum_t X_t^{(i)}
    $$
    这可以看作是对所有 $(s_t,a_t)$ 上的 log-ratio 的一次 Monte Carlo 估计：  
    **每个 step 是一个随机变量 $X_t$，先在序列内对 step 求和，再在 batch 上取平均。**
  - 和“对每个 step 单独算 KL 再求和/平均”的标准做法在数学上是一致的，只是这里的实现把“所有 step 的贡献”先用 `.sum()` 聚合进了 `actor_logp` / `old_logp` 这个标量，再参与后续计算；同时，本仓库进一步简化为整句共享同一个 advantage。

- **`kl` (Actor vs Old Actor)**：
  - **作用**：仅用于**监控**（WandB 日志）。
  - **含义**：这一步更新让策略偏离上一版多远？如果这个值太大，说明更新太剧烈，PPO 的 clip 机制可能被大量触发。
  
- **`kl_ref` (Actor vs Ref Model)**：
  - **作用**：加入到 **Loss** 中作为惩罚项。
  - **含义**：现在的模型偏离初始 SFT 模型多远？
  - **目的**：防止 RL 让模型“忘本”（遗忘通用的语言能力或 SFT 学到的格式），避免 Reward Hacking（为了高分输出乱码）。

#### Step 3: 计算 Advantage (简化版)

```python
# 代码中简化为：Final Reward - Final Value
advantages = rewards - values.detach() # [B]
```

- 这里没有使用 GAE，而是假设整个句子的贡献都来自最终的 Reward。
- `detach()` 很重要：计算 Advantage 时不需要对 Value Model 求导。

#### Step 4: 计算 Ratio 与 Surrogate Loss（PPO 核心）

```python
# 计算重要性采样比率 (Importance Sampling Ratio)
ratio = torch.exp(actor_logp - old_logp)  # [B]
```

$$
\text{ratio}_i = \frac{\pi_\theta(y^{(i)} \mid x^{(i)})}{\pi_{\text{old}}(y^{(i)} \mid x^{(i)})}
$$

接着构造两个 Loss 项：

```python
surr1 = ratio * advantages  # [B]
surr2 = torch.clamp(ratio, 1.0 - eps, 1.0 + eps) * advantages
```

- **surr1**：原始的 Policy Gradient 目标。
- **surr2**：将 Ratio 截断在 $[1-\epsilon, 1+\epsilon]$ 范围内。

最终的 Policy Loss 取二者较小值（的负数，因为要最大化）：

```python
policy_loss = -torch.min(surr1, surr2).mean()
```

**Clip 机制详解**：
- 当 Advantage $> 0$（想提高概率）：`min` 限制了 `ratio` 不能超过 $1+\epsilon$。**“这是好动作，别一下子提太猛。”**
- 当 Advantage $< 0$（想降低概率）：`min` 限制了 `ratio` 不能低于 $1-\epsilon$。**“这是坏动作，别一下子降没了。”**

#### Step 5: Value Loss (Critic 训练)

```python
value_loss = F.mse_loss(values, rewards)
```
- 使用均方误差（MSE）让 Critic 的预测值 `values` 逼近真实的 `rewards`。

#### Step 6: 最终 Total Loss

```python
loss = policy_loss + args.vf_coef * value_loss + args.kl_coef * kl_ref
```

总 Loss 由三部分组成：
1.  **Policy Loss**：优化 Actor，使其生成高分回复（受 Clip 约束）。
2.  **Value Loss**：优化 Critic，使其打分更准（权重 `vf_coef` 通常为 0.5）。
3.  **KL Penalty**：约束 Actor，使其不偏离 SFT 模型太远（权重 `kl_coef` 通常很小，如 0.02）。

# DPO (Direct Preference Optimization) 原理详解

DPO (Direct Preference Optimization) 是一种不需要训练独立的奖励模型（Reward Model）就能让大模型对齐人类偏好的算法。它通过直接优化策略模型，使其在“接受（Chosen）”和“拒绝（Rejected）”的数据对上满足偏好约束。

## 1. 背景回顾：PPO 在 RLHF 中的角色

为了理解 DPO 为什么优雅，我们需要先看看传统的 RLHF + PPO 流程是多么复杂。

典型的 RLHF 流程如下：

1.  **准备数据**：拿一堆 (prompt, response) 偏好对 $(x, y^+, y^-)$，人类标记更喜欢 $y^+$ 而不是 $y^-$。
2.  **训练 Reward Model (RM)**：
    *   训练一个模型 $r_\phi(x, y)$ 给回答打分。
    *   目标：让 $r_\phi(x, y^+) > r_\phi(x, y^-)$。
    *   损失函数：$L_{RM} = -\log \sigma(r_\phi(x, y^+) - r_\phi(x, y^-))$。
3.  **使用 PPO 训练 Policy (Actor)**：
    *   这是最繁琐的一步。
    *   **采样**：用当前策略 $\pi_\theta$ 生成回答 $y$ (On-policy rollout)。
    *   **打分**：用 RM 计算奖励，减去 KL 散度惩罚。
    *   **更新**：计算 Advantage，使用 PPO 的 Clip Loss 更新模型，同时还得训练一个 Critic ($V_\psi$) 模型。

**痛点**：需要维护 4 个模型（Actor, Critic, Ref, Reward），流程复杂，训练不稳定，计算开销大。

## 2. DPO 的直觉：化繁为简

DPO 的核心想法非常直接：
**既然我们本来就是想要“让策略在偏好对上更偏向好回答”，那能不能跳过 Reward Model 和 PPO 这些中间步骤，直接在数据上算 Loss？**

DPO 的数学推导证明了这是可行的。它发现：
> 在 KL 正则约束下的 RLHF 目标，最优解的形式是固定的。我们可以反推，把 Reward 函数用 Policy 模型本身来表示。

换句话说，DPO 把“训练 Reward Model”和“PPO 最大化 Reward”这两步，合并成了一个**监督式的二分类风格 Loss**。模型直接在偏好数据上更新，不需要 Critic，也不需要采样。

## 3. DPO 核心公式详解

我们设定：
*   $\pi_{ref}$：参考策略（通常是 SFT 后的模型，对应代码 `ref_model`）。
*   $\pi_{\theta}$：当前训练的策略（对应代码 `model`）。
*   数据：$(x, y^+, y^-)$，即 $(Prompt, Chosen, Rejected)$。

DPO 的 Loss (per pair) 定义为：

$$
L_{DPO}(\theta) = -\log \sigma \left( \beta \left[ \underbrace{\log \frac{\pi_\theta(y^+|x)}{\pi_\theta(y^-|x)}}_{\text{新模型的 Log 比}} - \underbrace{\log \frac{\pi_{ref}(y^+|x)}{\pi_{ref}(y^-|x)}}_{\text{参考模型的 Log 比}} \right] \right)
$$

为了方便理解，我们定义 $\Delta$ (Delta) 为 **“好回答 vs 坏回答”的 Log 概率差**：

$$
\Delta_\theta = \log \pi_\theta(y^+|x) - \log \pi_\theta(y^-|x)
$$
$$
\Delta_{ref} = \log \pi_{ref}(y^+|x) - \log \pi_{ref}(y^-|x)
$$

那么 Loss 简化为：

$$
L_{DPO}(\theta) = -\log \sigma( \beta (\Delta_\theta - \Delta_{ref}) )
$$

### 公式直觉拆解
*   **$\Delta_\theta$**：当前模型觉得“好回答”比“坏回答”好多少。我们希望这个值越大越好。
*   **$\Delta_{ref}$**：参考模型觉得“好回答”比“坏回答”好多少。这是一个基准线。
*   **$\Delta_\theta - \Delta_{ref}$**：**相对增量**。DPO 要求新模型对好回答的偏好程度，要**超过**参考模型。
*   **$\beta$**：控制系数。
    *   $\beta$ 越大，对偏离参考模型的惩罚越重（相当于 KL 约束强）。
    *   $\beta$ 越小，模型越自由。

这本质上就是一个**二分类 Logistic 回归**：
*   **输入**：这一对回答的相对 Log Prob 增量。
*   **标签**：人类选了“+”。

## 4. 代码实现与映射

在 `trainer/train_dpo.py` 中，`dpo_loss` 函数（第 33-51 行）直接实现了上述逻辑：

```python
def dpo_loss(ref_log_probs, policy_log_probs, mask, beta):
    # ...
    
    # 1. 计算 Delta_theta (新模型的偏好差)
    # chosen_policy_log_probs 对应 log \pi_theta(y+|x)
    # reject_policy_log_probs 对应 log \pi_theta(y-|x)
    pi_logratios = chosen_policy_log_probs - reject_policy_log_probs
    
    # 2. 计算 Delta_ref (参考模型的偏好差 - 基准线)
    ref_logratios = chosen_ref_log_probs - reject_ref_log_probs
    
    # 3. 计算相对增量 (Delta_theta - Delta_ref)
    logits = pi_logratios - ref_logratios
    
    # 4. 计算 Loss: -log(sigmoid(beta * logits))
    loss = -F.logsigmoid(beta * logits)
    return loss.mean()
```

## 5. 简单示例演示

假设客服场景：
*   **Prompt**: "我的快递丢了怎么办？"
*   **Chosen ($y^+$)**: "请提供单号，我帮您查。"
*   **Rejected ($y^-$)**: "不知道。"

### 场景模拟

**Step 1: 参考模型 (Ref) 的看法**
SFT 模型可能觉得两个回答差不多，甚至坏回答概率更高（因为它短）：
*   Log P(Chosen) = -2.0
*   Log P(Rejected) = -1.5
*   **$\Delta_{ref} = -2.0 - (-1.5) = -0.5$** (Ref 模型其实稍微更倾向于坏回答)

**Step 2: 训练模型 (Policy) 的看法**
经过训练，Policy 模型开始学好了：
*   Log P(Chosen) = -0.5 (概率提升)
*   Log P(Rejected) = -3.0 (概率下降)
*   **$\Delta_\theta = -0.5 - (-3.0) = +2.5$** (Policy 模型强烈倾向于好回答)

**Step 3: 计算 Loss**
*   **差异 (Logits)** = $\Delta_\theta - \Delta_{ref} = 2.5 - (-0.5) = 3.0$
    *   注意：哪怕 Ref 模型判断错了，只要 Policy 模型比它“更对”，Logits 就会是正的。
*   **Loss** = $-\log \sigma(0.1 \times 3.0) = -\log \sigma(0.3) \approx -\log(0.57) \approx 0.56$

随着训练进行，$\Delta_\theta$ 会越来越大，Logits 变大，Loss 变小，模型就对齐了人类偏好。

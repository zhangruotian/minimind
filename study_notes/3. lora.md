# LoRA (Low-Rank Adaptation) 实现总结

## 1. LoRA 原理

### 1.1 核心思想
LoRA 是一种参数高效的微调方法，通过在预训练模型的权重矩阵(linear层)旁添加低秩分解矩阵，只训练少量参数即可适应新任务。

### 1.2 数学表达
对于原始线性层 `W ∈ R^(d×k)`，LoRA 将其更新分解为：
```
W' = W + ΔW = W + BA
```
其中：
- `B ∈ R^(d×r)`：低秩矩阵 B（r << min(d,k)）
- `A ∈ R^(r×k)`：低秩矩阵 A
- `r`：秩（rank），控制低秩矩阵大小，通常为 8-64

前向传播：
```
h = Wx + BAx = Wx + B(Ax)
```

### 1.3 优势
- **参数效率**：只训练 `r×(d+k)` 个参数，而非 `d×k` 个
- **内存友好**：冻结原权重，只需保存少量 LoRA 参数
- **模块化**：不同任务可训练不同的 LoRA 适配器，灵活组合

## 2. 代码库实现

### 2.1 LoRA 模块结构 (`model/model_lora.py`)

```python
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank):
        self.A = nn.Linear(in_features, rank, bias=False)   # 降维
        self.B = nn.Linear(rank, out_features, bias=False)  # 升维
        
        # 初始化策略
        self.A.weight.data.normal_(mean=0.0, std=0.02)  # A: 高斯初始化
        self.B.weight.data.zero_()                      # B: 全零初始化
```

**初始化策略**：B 全零初始化确保训练初期 `ΔW = BA = 0`，模型行为与原始模型一致。

### 2.2 应用到模型

#### 应用条件
当前实现只应用到**输入输出维度相同的 Linear 层**：
```python
if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
```

**注意**：这主要覆盖了 `o_proj` 等层，但标准的 LoRA 实现通常也会应用到 `q_proj`, `k_proj`, `v_proj` 等 Attention 层。

#### 应用机制
```python
def apply_lora(model, rank=8):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:
            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank)
            setattr(module, "lora", lora)  # 注册为子模块
            
            # 修改前向传播：原始输出 + LoRA 输出
            original_forward = module.forward
            def forward_with_lora(x, layer1=original_forward, layer2=lora):
                return layer1(x) + layer2(x)
            module.forward = forward_with_lora
```

**关键机制**：
- `setattr(module, "lora", lora)` 会将 `LoRA` 对象（继承自 `nn.Module`）自动注册为子模块
- PyTorch 的 `__setattr__` 会检测 `nn.Module` 类型并加入 `_modules` 字典
- 因此 `model.named_parameters()` 会递归遍历，参数名包含 `"lora"` 路径

### 2.3 参数管理

#### 参数统计与分离
```python
# 统计总参数和 LoRA 参数
total_params = sum(p.numel() for p in model.parameters())
lora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name)

# 分离 LoRA 参数，冻结其他参数
lora_params = []
for name, param in model.named_parameters():
    if 'lora' in name:
        param.requires_grad = True
        lora_params.append(param)
    else:
        param.requires_grad = False
```

#### 优化器初始化
```python
optimizer = optim.AdamW(lora_params, lr=args.learning_rate)
```

**为什么显式传入 `lora_params`？**
- **内存效率**：只为 LoRA 参数创建优化器状态（momentum、variance 等）
- **明确性**：代码意图清晰，只优化 LoRA 参数
- **安全性**：即使某些参数 `requires_grad` 设置错误，也不会被优化

**对比**：传入 `model.parameters()` 也可以工作（optimizer 会自动过滤 `requires_grad=False`），但会为所有参数创建状态字典，浪费内存。

### 2.4 保存与加载

#### 保存 LoRA 权重
```python
def save_lora(model, path):
    state_dict = {}
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            # 保存格式：{module_path}.lora.{A/B}.weight
            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()}
            state_dict.update(lora_state)
    torch.save(state_dict, path)
```

#### 加载 LoRA 权重
```python
def load_lora(model, path):
    state_dict = torch.load(path, map_location=model.device)
    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            # 从完整路径中提取对应模块的 LoRA 状态
            lora_state = {k.replace(f'{name}.lora.', ''): v 
                          for k, v in state_dict.items() 
                          if f'{name}.lora.' in k}
            module.lora.load_state_dict(lora_state)
```

**特点**：只保存/加载 LoRA 参数，不包含原始模型权重，实现轻量级适配器存储。

## 3. 训练流程 (`trainer/train_lora.py`)

### 3.1 训练步骤
1. **初始化模型**：加载预训练权重（如 `full_sft`）
2. **应用 LoRA**：调用 `apply_lora(model)` 为符合条件的层添加 LoRA 适配器
3. **冻结参数**：设置非 LoRA 参数 `requires_grad=False`
4. **初始化优化器**：只传入 LoRA 参数列表
5. **训练循环**：标准的前向传播、反向传播、参数更新

### 3.2 前向传播
```python
res = model(X)  # 自动使用修改后的 forward_with_lora
loss = loss_fct(res.logits.view(-1, res.logits.size(-1)), Y.view(-1))
loss = (loss * loss_mask).sum() / loss_mask.sum()
loss += res.aux_loss  # 如果有 MoE 辅助损失
```

### 3.3 梯度更新
```python
scaler.scale(loss).backward()
if (step + 1) % args.accumulation_steps == 0:
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip)  # 只裁剪 LoRA 参数
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad(set_to_none=True)
```

## 4. 关键设计点

### 4.1 模块注册机制
- `setattr(module, "lora", lora)` 会自动注册 `LoRA` 为子模块
- `named_parameters()` 递归遍历，参数名包含完整路径（如 `model.layers.0.self_attn.q_proj.lora.A.weight`）
- 可通过 `'lora' in name` 筛选 LoRA 参数

### 4.2 参数维度处理
- `nn.Linear` 的 `weight` 形状为 `[out_features, in_features]`
- 创建 `LoRA` 时需注意：`LoRA(in_features, out_features, rank)`
- 当前实现使用 `module.weight.shape[0]` 作为维度（假设输入输出相同）

### 4.3 前向传播修改
- 通过替换 `module.forward` 实现：`original_output + lora_output`
- 使用闭包捕获 `original_forward` 和 `lora` 对象
- 保持原始权重不变，LoRA 作为增量更新


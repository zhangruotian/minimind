# 8. LLM 服务化部署与接口兼容

本文档记录了如何将 MiniMind 模型部署为兼容 OpenAI API 协议的服务，以及如何通过客户端进行调用。

## 1. 服务端方案 A：使用 Python 原生部署 (`serve_openai_api.py`)

该脚本的核心作用是**使用 FastAPI 部署本地模型，并模拟 OpenAI API 的输入输出格式**。

### 1.1 核心原理
- **FastAPI 服务**: 使用 FastAPI 框架启动 HTTP 服务。
- **本地推理**: 直接加载本地训练好的 MiniMind 模型 (`.pth` 或 Transformers 格式) 进行推理，不依赖外部 OpenAI 服务。
- **接口伪装**: 路由、参数名、返回结构完全复刻 OpenAI 标准，使得现有工具可以直接无缝接入。

### 1.2 关键代码解析

**API 路由定义**
```python
# 定义与 OpenAI 一致的接口路径 (serve_openai_api.py)
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    # ...
```

**流式响应 (Streaming)**
为了实现打字机效果，服务端使用生成器并封装为 `StreamingResponse`：
```python
if request.stream:
    return StreamingResponse(
        (f"data: {chunk}\n\n" for chunk in generate_stream_response(...)),
        media_type="text/event-stream"
    )
```

**伪造响应格式**
返回的 JSON 结构严格遵循 OpenAI 定义，包含 `id`, `object`, `choices` 等字段：
```python
return {
    "id": f"chatcmpl-{int(time.time())}",
    "object": "chat.completion",
    "model": "minimind",
    "choices": [
        {
            "index": 0,
            "message": {"role": "assistant", "content": answer},
            "finish_reason": "stop"
        }
    ]
}
```

---

## 2. 服务端方案 B：使用 vLLM 部署 (推荐)

vLLM 是一个高性能的 LLM 推理和服务库，支持极高的吞吐量和低延迟。它原生内置了 OpenAI 兼容的 API 服务器。

### 2.1 为什么选择 vLLM
- **高性能**: 吞吐量比 HuggingFace Transformers 高出数倍。
- **显存优化**: PagedAttention 算法高效管理显存。
- **原生兼容**: 启动命令即自带 OpenAI 兼容接口，无需手写 FastAPI 代码。

### 2.2 启动命令
确保模型已转换为 HuggingFace Transformers 格式（包含 `config.json`, `tokenizer.json`, `model.safetensors` 等）。

```bash
python -m vllm.entrypoints.openai.api_server \
    --model ./MiniMind2 \
    --served-model-name minimind \
    --port 8998 \
    --trust-remote-code
```

或者使用简化的 `vllm serve` 命令（v0.4.0+）：
```bash
vllm serve ./MiniMind2 --served-model-name minimind --port 8998 --trust-remote-code
```

- `--model`: 指向转换后的模型目录。
- `--served-model-name`: 设置对外暴露的模型名称（客户端调用时传的 `model` 参数）。
- `--port`: 服务端口。
- `--trust-remote-code`: 如果模型包含自定义架构代码，需要开启此选项。

---

## 3. 客户端：使用 OpenAI SDK 调用 (`chat_openai_api.py`)

该脚本展示了如何使用官方 `openai` Python 库连接本地服务。它相当于一个简易的命令行聊天机器人前端。

### 3.1 连接配置
最关键的一步是修改 `base_url`，让 SDK 连接本地端口而不是 OpenAI 服务器。

```python
from openai import OpenAI

client = OpenAI(
    api_key="ollama",                # 服务端未做校验，可任意填写
    base_url="http://127.0.0.1:8998/v1"  # 指向本地 FastAPI 服务地址
)
```

### 3.2 上下文记忆管理
由于 HTTP 请求是无状态的，客户端负责维护“记忆”：
1. 这是一个列表 `conversation_history`，存储所有的 `user` 和 `assistant` 消息。
2. 每次请求时，将最近的历史记录切片发送给服务器。

```python
# 截取最近 history_messages_num 条消息发送
messages = conversation_history[-history_messages_num:]
```

### 3.3 流式接收
对应服务端的流式响应，客户端逐块接收并打印：
```python
for chunk in response:
    # 实时获取增量内容 (delta)
    content = chunk.choices[0].delta.content or ""
    print(content, end="") # end="" 确保不换行，形成打字效果
```

---

## 4. 生态对接价值

这种兼容性设计的最大价值在于 **"即插即用"**。任何支持 OpenAI API 的第三方生态工具都可以直接驱动你的 MiniMind 模型。

**对接通用配置：**
- **Base URL (API 域名)**: `http://localhost:8998/v1`
- **API Key (密钥)**: `sk-xxxx` (任意填写)
- **Model (模型名)**: `minimind` (任意填写，服务端通常只加载了一个模型)

**适用场景：**
- **前端界面**: Chatbox, NextChat (ChatGPT-Next-Web)
- **开发框架**: LangChain, AutoGPT, LlamaIndex
- **插件工具**: 沉浸式翻译, VSCode 插件等

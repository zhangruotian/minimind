---
pipeline_tag: text-classification
license: other
language:
- en
- zh
tags:
- reward model
---
# InternLM 

<div align="center">

<img src="https://github.com/InternLM/InternLM/assets/22529082/b9788105-8892-4398-8b47-b513a292378e" width="200"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="5">InternLM2-1.8B-Reward</font></b>
  </div>
  

[ğŸ’»Github Repo](https://github.com/InternLM/InternLM) â€¢ [ğŸ¤”Reporting Issues](https://github.com/InternLM/InternLM/issues/new) â€¢ [ğŸ“œTechnical Report](https://arxiv.org/abs/2403.17297)

</div>

<p align="center">
    ğŸ‘‹ join us on <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> and <a href="https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce" target="_blank">WeChat</a>
</p>


## Introduction

**InternLM2-1.8B-Reward** is a reward model trained on the foundation of InternLM2-Chat-1.8B-SFT. This model has been trained using over 2.4 million preference samples, both human-annotated and AI-synthesized, achieving outstanding performance while ensuring a balance between helpful and harmless. 

### Key Features:
- **Variety of Sizes Available**: Our open-sourced reward models are available in sizes of **1.8B, 7B, and 20B**, each demonstrating exceptional performance across various metrics. We aim for these different-sized models to facilitate research on the scaling laws of reward models, providing valuable insights to the community.
- **Comprehensive Coverage of Preference**: Trained with **2.4 million** preference pairs derived from both human annotations and AI synthesis, covering diverse areas such as dialogue, writing, poetry, summarization, coding, mathematics, etc. It also maintains a balance between helpful and harmless.
- **Multilingual Support**: InternLM2-Reward was trained on high-quality **English and Chinese** preference data, delivering robust performance in both languages.

This model was applied to the RLHF training process of InternLM2-Chat. The reward model training techniques from the [InternLM2 Technical Report](https://arxiv.org/abs/2403.17297) have been open-sourced in XTuner, try it out [here](https://github.com/InternLM/xtuner)!

## Performance Evaluation on RewardBench

| Models | Score | Chat | Chat Hard | Safety | Reasoning |
| --- | --- | --- | --- | --- | --- |
| InternLM2-20B-Reward | 89.5 | 98.6 | 74.1 | 89.4 | 95.7 |
| InternLM2-7B-Reward | 86.6 | 98.6 | 66.7 | 88.3 | 92.8 |
| InternLM2-1.8B-Reward | 80.6 | 95.0 | 58.1 | 81.8 | 87.4 |

- The evaluation is conducted on the [RewardBench](https://github.com/allenai/reward-bench) dataset.
- For a fair comparison, conditional system prompts proposed in our technical report were not included during testing.

## Demo Code

### Basic Usage

We provide some user-friendly APIs for you to use the model. Here is an example of how to use the model to get the reward score of a chat, compare two chats, or rank multiple chats.

```python
import torch
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained(
    "internlm/internlm2-1_8b-reward", 
    device_map="cuda", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-1_8b-reward", trust_remote_code=True)

chat_1 = [
    {"role": "user", "content": "Hello! What's your name?"},
    {"role": "assistant", "content": "My name is InternLM2! A helpful AI assistant. What can I do for you?"}
]
chat_2 = [
    {"role": "user", "content": "Hello! What's your name?"}, 
    {"role": "assistant", "content": "I have no idea."}
]


# get reward score for a single chat
score1 = model.get_score(tokenizer, chat_1)
score2 = model.get_score(tokenizer, chat_2)
print("score1: ", score1)
print("score2: ", score2)
# >>> score1:  0.767578125
# >>> score2:  -2.22265625


# batch inference, get multiple scores at once
scores = model.get_scores(tokenizer, [chat_1, chat_2])
print("scores: ", scores)
# >>> scores:  [0.767578125, -2.22265625]


# compare whether chat_1 is better than chat_2
compare_res = model.compare(tokenizer, chat_1, chat_2)
print("compare_res: ", compare_res)
# >>> compare_res:  True


# rank multiple chats, it will return the ranking index of each chat
# the chat with the highest score will have ranking index as 0 
rank_res = model.rank(tokenizer, [chat_1, chat_2])
print("rank_res: ", rank_res)  # lower index means higher score
# >>> rank_res:  [0, 1]  
```

### Best of N Sampling

Here is an example of how to use the reward model to perform best of N sampling. 
The code below demonstrates how to select the best response from the candidates generated by the language model.

```python
import torch
from transformers import AutoModel, AutoTokenizer

# prepare the llm model and tokenizer
llm = AutoModel.from_pretrained(
    "internlm/internlm2-chat-7b",
    device_map="cuda", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
)
llm_tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-chat-7b", trust_remote_code=True)

# prepare the reward model and tokenizer
reward = AutoModel.from_pretrained(
    "internlm/internlm2-1_8b-reward",
    device_map="cuda", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
)
reward_tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-1_8b-reward", trust_remote_code=True)

# prepare the chat prompt
prompt = "Write an article about the artificial intelligence revolution."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = llm_tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = llm_tokenizer([text], return_tensors="pt").to("cuda")

# generate best of N candidates
num_candidates = 10  # N=10
candidates = []

outputs = llm.generate(
    **model_inputs,
    max_new_tokens=512,
    num_return_sequences=num_candidates,
    pad_token_id=llm_tokenizer.eos_token_id,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.8,
)
outputs = outputs[:, model_inputs["input_ids"].shape[1]:]
for i in range(num_candidates):
    candidate = llm_tokenizer.decode(outputs[i], skip_special_tokens=True)
    candidates.append(messages + [{"role": "assistant", "content": candidate}])

rank_indices = reward.rank(reward_tokenizer, candidates)
sorted_candidates = sorted(zip(rank_indices, candidates), key=lambda x: x[0])

## print the ranked candidates
# for i, (rank_index, candidate) in enumerate(sorted_candidates):
#     print(f"------------Rank {i}------------: \n{candidate[-1]['content']}")

# print the best response
best_response = sorted_candidates[0][1][-1]['content']
print(best_response)
```

## Open Source License
The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow **free** commercial usage. To apply for a commercial license, please fill in the [application form (English)](https://wj.qq.com/s2/12727483/5dba/)/[ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰](https://wj.qq.com/s2/12725412/f7c1/). For other questions or collaborations, please contact <internlm@pjlab.org.cn>.
## Citation
```
@misc{cai2024internlm2,
      title={InternLM2 Technical Report},
      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},
      year={2024},
      eprint={2403.17297},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
## ç®€ä»‹

**InternLM2-1.8B-Reward** æ˜¯åŸºäº **InternLM2-Chat-1.8B-SFT** è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨è¶…è¿‡ 240 ä¸‡æ¡äººå·¥æ ‡æ³¨å’Œ AI åˆæˆçš„åå¥½æ ·æœ¬ï¼Œè¦†ç›–äº†åŒ…æ‹¬å¯¹è¯ã€å†™ä½œã€è¯—æ­Œã€æ€»ç»“ã€ç¼–ç å’Œæ•°å­¦ç­‰å¤šä¸ªé¢†åŸŸã€‚åœ¨å–å¾—äº†å‡ºè‰²æ€§èƒ½çš„åŒæ—¶ä¹Ÿå…¼é¡¾äº†å®ç”¨æ€§å’Œå®‰å…¨æ€§åå¥½çš„å¹³è¡¡ã€‚

### InternLM2-Reward çš„ä¸»è¦ç‰¹ç‚¹ï¼š
- **å¤šç§å°ºå¯¸å¯ä¾›é€‰æ‹©**ï¼šæˆ‘ä»¬å¼€æºçš„å¥–åŠ±æ¨¡å‹æœ‰ **1.8Bã€7B å’Œ 20B** ä¸‰ç§å°ºå¯¸ï¼Œæ¯ç§å°ºå¯¸éƒ½å±•ç¤ºå‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›ä¸åŒå¤§å°çš„æ¨¡å‹èƒ½å¤Ÿä¿ƒè¿›ç¤¾åŒºå…³äº Reward Model ç¼©æ”¾å®šå¾‹çš„ç ”ç©¶ã€‚
- **å…¨é¢è¦†ç›–åå¥½**ï¼šæ¨¡å‹è®­ç»ƒäº† **240 ä¸‡**æ¡æ¥è‡ªäººå·¥æ ‡æ³¨å’ŒAIåˆæˆçš„åå¥½æ ·æœ¬ï¼Œæ¶‰åŠå¯¹è¯ã€å†™ä½œã€è¯—æ­Œã€æ€»ç»“ã€ç¼–ç å’Œæ•°å­¦ç­‰å¤šä¸ªé¢†åŸŸï¼ŒåŒæ—¶ç¡®ä¿äº†å®ç”¨æ€§å’Œå®‰å…¨æ€§åå¥½çš„å¹³è¡¡ã€‚
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šInternLM2-Reward åœ¨é«˜è´¨é‡çš„**è‹±æ–‡å’Œä¸­æ–‡**åå¥½æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç¡®ä¿äº†åœ¨è¿™ä¸¤ç§è¯­è¨€ä¸Šéƒ½æœ‰ç¨³å¥çš„è¡¨ç°ã€‚

è¯¥æ¨¡å‹è¿ç”¨åœ¨äº† InternLM2-Chat çš„ PPO è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2403.17297)ä¸­æå‡ºçš„ Reward Model è®­ç»ƒæŠ€å·§å·²åœ¨ XTuner ä¸­å…¬å¼€ã€‚æ¬¢è¿ç‚¹å‡»[é“¾æ¥](https://github.com/InternLM/xtuner)è¿›è¡Œå°è¯•ï¼

## RewardBench ä¸Šçš„æ€§èƒ½è¯„ä¼°

| Models | Score | Chat | Chat Hard | Safety | Reasoning |
| --- | --- | --- | --- | --- | --- |
| InternLM2-20B-Reward | 89.5 | 98.6 | 74.1 | 89.4 | 95.7 |
| InternLM2-7B-Reward | 86.6 | 98.6 | 66.7 | 88.3 | 92.8 |
| InternLM2-1.8B-Reward | 80.6 | 95.0 | 58.1 | 81.8 | 87.4 |

- è¯„ä¼°ä½¿ç”¨äº† [RewardBench](https://github.com/allenai/reward-bench) æ•°æ®é›†è¿›è¡Œã€‚
- ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæµ‹è¯•æœŸé—´æ²¡æœ‰ä½¿ç”¨æˆ‘ä»¬æŠ€æœ¯æŠ¥å‘Šä¸­æå‡ºçš„"æ¡ä»¶ç³»ç»Ÿæç¤º"ã€‚

## ç¤ºä¾‹ä»£ç 

### åŸºæœ¬ç”¨æ³•

æˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†ä¸€äº›ç”¨æˆ·å‹å¥½çš„ API ä»¥ä¾¿ä½¿ç”¨è¯¥æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ InternLM2-Reward è·å–å¯¹è¯çš„å¥–åŠ±åˆ†æ•°ã€æ¯”è¾ƒä¸¤ç»„å¯¹è¯æˆ–å¯¹å¤šä¸ªå¯¹è¯è¿›è¡Œæ’åã€‚

```python
import torch
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained(
    "internlm/internlm2-1_8b-reward", 
    device_map="cuda", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-1_8b-reward", trust_remote_code=True)

chat_1 = [
    {"role": "user", "content": "Hello! What's your name?"},
    {"role": "assistant", "content": "My name is InternLM2! A helpful AI assistant. What can I do for you?"}
]
chat_2 = [
    {"role": "user", "content": "Hello! What's your name?"}, 
    {"role": "assistant", "content": "I have no idea."}
]


# è·å–å•ä¸ªå¯¹è¯çš„å¥–åŠ±åˆ†æ•°
score1 = model.get_score(tokenizer, chat_1)
score2 = model.get_score(tokenizer, chat_2)
print("score1: ", score1)
print("score2: ", score2)
# >>> score1:  0.767578125
# >>> score2:  -2.22265625


# æ‰¹é‡æ¨ç†ï¼Œä¸€æ¬¡è·å–å¤šä¸ªåˆ†æ•°
scores = model.get_scores(tokenizer, [chat_1, chat_2])
print("scores: ", scores)
# >>> scores:  [0.767578125, -2.22265625]


# æ¯”è¾ƒ chat_1 æ˜¯å¦æ¯” chat_2 æ›´å¥½
compare_res = model.compare(tokenizer, chat_1, chat_2)
print("compare_res: ", compare_res)
# >>> compare_res:  True


# æ’åå¤šä¸ªå¯¹è¯ï¼Œå®ƒå°†è¿”å›æ¯ä¸ªå¯¹è¯çš„æ’ååºå·
# åˆ†æ•°æœ€é«˜çš„å¯¹è¯æ’ååºå·ä¸º 0
rank_res = model.rank(tokenizer, [chat_1, chat_2])
print("rank_res: ", rank_res)  # æ’ååºå·è¶Šä½è¡¨ç¤ºåˆ†æ•°è¶Šé«˜
# >>> rank_res:  [0, 1]  
```

### Best of N é‡‡æ ·

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ InternLM2-Reward æ‰§è¡ŒBest of N é‡‡æ ·çš„ç¤ºä¾‹ã€‚
ä»¥ä¸‹ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä»è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å€™é€‰å›ç­”ä¸­é€‰æ‹©æœ€ä½³å›ç­”ã€‚

```python
import torch
from transformers import AutoModel, AutoTokenizer

# å‡†å¤‡è¯­è¨€æ¨¡å‹å’Œåˆ†è¯å™¨
llm = AutoModel.from_pretrained(
    "internlm/internlm2-chat-7b",
    device_map="cuda", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
)
llm_tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-chat-7b", trust_remote_code=True)

# å‡†å¤‡å¥–åŠ±æ¨¡å‹å’Œåˆ†è¯å™¨
reward = AutoModel.from_pretrained(
    "internlm/internlm2-1_8b-reward",
    device_map="cuda", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
)
reward_tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-1_8b-reward", trust_remote_code=True)

# å‡†å¤‡æç¤ºè¯
prompt = "Write an article about the artificial intelligence revolution."
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = llm_tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = llm_tokenizer([text], return_tensors="pt").to("cuda")

# ç”Ÿæˆ N ä¸ªå€™é€‰
num_candidates = 10  # N=10
candidates = []

outputs = llm.generate(
    **model_inputs,
    max_new_tokens=512,
    num_return_sequences=num_candidates,
    pad_token_id=llm_tokenizer.eos_token_id,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.8,
)
outputs = outputs[:, model_inputs["input_ids"].shape[1]:]


for i in range(num_candidates):
    candidate = llm_tokenizer.decode(outputs[i], skip_special_tokens=True)
    candidates.append(messages + [{"role": "assistant", "content": candidate}])

rank_indices = reward.rank(reward_tokenizer, candidates)
sorted_candidates = sorted(zip(rank_indices, candidates), key=lambda x: x[0])

## æ‰“å°æ’åºåçš„å€™é€‰
# for i, (rank_index, candidate) in enumerate(sorted_candidates):
#     print(f"------------Rank {i}------------: \n{candidate[-1]['content']}")

# æ‰“å°æœ€ä½³å›ç­”
best_response = sorted_candidates[0][1][-1]['content']
print(best_response)
```

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚

## å¼•ç”¨

```
@misc{cai2024internlm2,
      title={InternLM2 Technical Report},
      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},
      year={2024},
      eprint={2403.17297},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```